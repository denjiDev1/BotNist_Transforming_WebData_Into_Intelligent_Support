{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Downloading Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-14T16:17:52.175718Z",
     "iopub.status.busy": "2025-06-14T16:17:52.175230Z",
     "iopub.status.idle": "2025-06-14T16:22:08.446464Z",
     "shell.execute_reply": "2025-06-14T16:22:08.445640Z",
     "shell.execute_reply.started": "2025-06-14T16:17:52.175686Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: huggingface-hub 0.31.1\n",
      "Uninstalling huggingface-hub-0.31.1:\n",
      "  Successfully uninstalled huggingface-hub-0.31.1\n",
      "Collecting huggingface_hub==0.15.1\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.15.1) (3.18.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.15.1) (2025.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.15.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.15.1) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.15.1) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.15.1) (4.13.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.15.1) (25.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.15.1) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.15.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.15.1) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.15.1) (2025.4.26)\n",
      "Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface_hub\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\n",
      "datasets 3.6.0 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "transformers 4.51.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "diffusers 0.32.2 requires huggingface-hub>=0.23.2, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "sentence-transformers 3.4.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "tokenizers 0.21.1 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "accelerate 1.5.2 requires huggingface-hub>=0.21.0, but you have huggingface-hub 0.15.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface_hub-0.15.1\n",
      "Collecting pydantic-settings==2.0.3\n",
      "  Downloading pydantic_settings-2.0.3-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: pydantic>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings==2.0.3) (2.11.4)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings==2.0.3)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.1->pydantic-settings==2.0.3) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.1->pydantic-settings==2.0.3) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.1->pydantic-settings==2.0.3) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.1->pydantic-settings==2.0.3) (0.4.0)\n",
      "Downloading pydantic_settings-2.0.3-py3-none-any.whl (11 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, pydantic-settings\n",
      "Successfully installed pydantic-settings-2.0.3 python-dotenv-1.1.0\n",
      "Collecting transformers==4.33.0\n",
      "  Downloading transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate==0.22.0\n",
      "  Downloading accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting einops==0.6.1\n",
      "  Downloading einops-0.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting langchain==0.0.300\n",
      "  Downloading langchain-0.0.300-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting xformers==0.0.21\n",
      "  Downloading xformers-0.0.21-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting bitsandbytes==0.41.1\n",
      "  Downloading bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting sentence_transformers==2.2.2\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting chromadb==0.4.15\n",
      "  Downloading chromadb-0.4.15-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.33.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.33.0) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.33.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.33.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.33.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.33.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.33.0) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.0)\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.33.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.33.0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.22.0) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.22.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.0.300) (2.0.40)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.0.300) (3.11.18)\n",
      "Collecting anyio<4.0 (from langchain==0.0.300)\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain==0.0.300) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain==0.0.300) (1.33)\n",
      "Collecting langsmith<0.1.0,>=0.0.38 (from langchain==0.0.300)\n",
      "  Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.0.300) (2.10.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.0.300) (2.11.4)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.300)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting torch>=1.10.0 (from accelerate==0.22.0)\n",
      "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from sentence_transformers==2.2.2) (0.21.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers==2.2.2) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers==2.2.2) (1.15.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from sentence_transformers==2.2.2) (3.9.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from sentence_transformers==2.2.2) (0.2.0)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb==0.4.15)\n",
      "  Downloading chroma_hnswlib-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb==0.4.15)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb==0.4.15)\n",
      "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb==0.4.15)\n",
      "  Downloading posthog-4.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.15) (4.13.2)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb==0.4.15)\n",
      "  Downloading pulsar_client-3.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb==0.4.15)\n",
      "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.15) (1.31.1)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb==0.4.15)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.15) (1.31.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb==0.4.15)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.15) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.15) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.15) (1.72.0rc1)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb==0.4.15)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.15) (0.15.2)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb==0.4.15)\n",
      "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.22.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.22.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=1.10.0->accelerate==0.22.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=1.10.0->accelerate==0.22.0)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch>=1.10.0->accelerate==0.22.0)\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=1.10.0->accelerate==0.22.0)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.10.0->accelerate==0.22.0)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch>=1.10.0->accelerate==0.22.0)\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch>=1.10.0->accelerate==0.22.0)\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch>=1.10.0->accelerate==0.22.0)\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch>=1.10.0->accelerate==0.22.0)\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch>=1.10.0->accelerate==0.22.0)\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch>=1.10.0->accelerate==0.22.0)\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.0.0 (from torch>=1.10.0->accelerate==0.22.0)\n",
      "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (75.2.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (0.45.1)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0) (3.31.6)\n",
      "Collecting lit (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0)\n",
      "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<4.0->langchain==0.0.300) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<4.0->langchain==0.0.300) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (0.9.0)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.95.2->chromadb==0.4.15)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2025.3.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.300) (3.0.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (2025.4.26)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (2.40.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (2.4.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb==0.4.15)\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.33.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.33.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.33.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.33.0) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.33.0) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.33.0) (2.4.1)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.15)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.15) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.15) (3.20.3)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.15) (1.2.18)\n",
      "Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb==0.4.15)\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.15) (1.70.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.15)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.15)\n",
      "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb==0.4.15)\n",
      "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb==0.4.15)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb==0.4.15)\n",
      "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.2.0->chromadb==0.4.15)\n",
      "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.15) (8.7.0)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.4.15)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.4.15) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.0.300) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.0.300) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.0.300) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.33.0) (3.4.2)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.300) (3.1.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.15) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.15) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.15) (14.0.0)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.15) (0.14.0)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb==0.4.15)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.15) (1.1.0)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb==0.4.15)\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.15)\n",
      "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.15) (15.0.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->sentence_transformers==2.2.2) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers==2.2.2) (3.6.0)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from sentence_transformers==2.2.2)\n",
      "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading torchvision-0.20.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "  Downloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "INFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torchvision-0.18.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.17.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.17.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading torchvision-0.16.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.16.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.16.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->sentence_transformers==2.2.2) (11.1.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.15) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.15) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.15) (4.9.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.4.15) (3.21.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.15) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.15) (2.19.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (1.1.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.15)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.33.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.33.0) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.33.0) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.33.0) (2024.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate==0.22.0) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.33.0) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.15) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.15) (0.6.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb==0.4.15) (1.17.2)\n",
      "Downloading transformers-4.33.0-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.0.300-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.21-cp311-cp311-manylinux2014_x86_64.whl (167.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading chromadb-0.4.15-py3-none-any.whl (479 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.8/479.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading posthog-4.10.0-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.5/102.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pulsar_client-3.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sentence_transformers, pypika\n",
      "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=a709261e32d6fa1348a4effecf359d9657b1e415ac1db1b5b7b492bcb80eef74\n",
      "  Stored in directory: /root/.cache/pip/wheels/ff/27/bf/ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=8057b7fac00d3b7f71b826ba950504d23b1ff527e68307f05854ddcd9e5a3b4f\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built sentence_transformers pypika\n",
      "Installing collected packages: tokenizers, pypika, lit, durationpy, bitsandbytes, uvloop, uvicorn, tenacity, pulsar-client, protobuf, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, humanfriendly, httptools, einops, bcrypt, backoff, anyio, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, nvidia-cusolver-cu11, nvidia-cudnn-cu11, coloredlogs, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, langsmith, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, triton, torch, transformers, torchvision, onnxruntime, chroma-hnswlib, xformers, sentence_transformers, langchain, chromadb, accelerate\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.1.2\n",
      "    Uninstalling tenacity-9.1.2:\n",
      "      Successfully uninstalled tenacity-9.1.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: einops\n",
      "    Found existing installation: einops 0.8.1\n",
      "    Uninstalling einops-0.8.1:\n",
      "      Successfully uninstalled einops-0.8.1\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.9.0\n",
      "    Uninstalling anyio-4.9.0:\n",
      "      Successfully uninstalled anyio-4.9.0\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.31.1\n",
      "    Uninstalling opentelemetry-api-1.31.1:\n",
      "      Successfully uninstalled opentelemetry-api-1.31.1\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.52b1\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.52b1:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.52b1\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.3.23\n",
      "    Uninstalling langsmith-0.3.23:\n",
      "      Successfully uninstalled langsmith-0.3.23\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.31.1\n",
      "    Uninstalling opentelemetry-sdk-1.31.1:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.31.1\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cu124\n",
      "    Uninstalling torch-2.6.0+cu124:\n",
      "      Successfully uninstalled torch-2.6.0+cu124\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.21.0+cu124\n",
      "    Uninstalling torchvision-0.21.0+cu124:\n",
      "      Successfully uninstalled torchvision-0.21.0+cu124\n",
      "  Attempting uninstall: sentence_transformers\n",
      "    Found existing installation: sentence-transformers 3.4.1\n",
      "    Uninstalling sentence-transformers-3.4.1:\n",
      "      Successfully uninstalled sentence-transformers-3.4.1\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.22\n",
      "    Uninstalling langchain-0.3.22:\n",
      "      Successfully uninstalled langchain-0.3.22\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.5.2\n",
      "    Uninstalling accelerate-1.5.2:\n",
      "      Successfully uninstalled accelerate-1.5.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\n",
      "pytorch-lightning 2.5.1.post0 requires torch>=2.1.0, but you have torch 2.0.1 which is incompatible.\n",
      "kaggle-environments 1.16.11 requires transformers>=4.33.1, but you have transformers 4.33.0 which is incompatible.\n",
      "langchain-core 0.3.50 requires langsmith<0.4,>=0.1.125, but you have langsmith 0.0.92 which is incompatible.\n",
      "langchain-core 0.3.50 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\n",
      "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "google-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-genai 1.9.0 requires anyio<5.0.0,>=4.8.0, but you have anyio 3.7.1 which is incompatible.\n",
      "pandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.22.0 anyio-3.7.1 backoff-2.2.1 bcrypt-4.3.0 bitsandbytes-0.41.1 chroma-hnswlib-0.7.3 chromadb-0.4.15 coloredlogs-15.0.1 durationpy-0.10 einops-0.6.1 fastapi-0.115.12 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 langchain-0.0.300 langsmith-0.0.92 lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 posthog-4.10.0 protobuf-5.29.5 pulsar-client-3.7.0 pypika-0.48.9 sentence_transformers-2.2.2 starlette-0.46.2 tenacity-8.5.0 tokenizers-0.13.3 torch-2.0.1 torchvision-0.15.2 transformers-4.33.0 triton-2.0.0 uvicorn-0.34.3 uvloop-0.21.0 watchfiles-1.0.5 xformers-0.0.21\n",
      "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
      "Collecting llama_index\n",
      "  Downloading llama_index-0.12.42-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting llama-index-agent-openai<0.5,>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_agent_openai-0.4.11-py3-none-any.whl.metadata (439 bytes)\n",
      "Collecting llama-index-cli<0.5,>=0.4.2 (from llama_index)\n",
      "  Downloading llama_index_cli-0.4.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting llama-index-core<0.13,>=0.12.42 (from llama_index)\n",
      "  Downloading llama_index_core-0.12.42-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4,>=0.3.0 (from llama_index)\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.7-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-llms-openai<0.5,>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_llms_openai-0.4.5-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.6,>=0.5.0 (from llama_index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl.metadata (440 bytes)\n",
      "Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama_index)\n",
      "  Downloading llama_index_program_openai-0.3.2-py3-none-any.whl.metadata (473 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama_index)\n",
      "  Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl.metadata (492 bytes)\n",
      "Collecting llama-index-readers-file<0.5,>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_readers_file-0.4.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama_index) (3.9.1)\n",
      "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.70.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (3.11.18)\n",
      "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (0.21.0)\n",
      "Collecting banks<3,>=2.0.0 (from llama-index-core<0.13,>=0.12.42->llama_index)\n",
      "  Downloading banks-2.1.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (1.2.18)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.13,>=0.12.42->llama_index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.13,>=0.12.42->llama_index)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (2025.3.2)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (3.4.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (2.11.4)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (2.32.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.42->llama_index) (2.0.40)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (4.13.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13,>=0.12.42->llama_index) (1.17.2)\n",
      "Collecting llama-cloud==0.1.26 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud-0.1.26-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud==0.1.26->llama-index-indices-managed-llama-cloud>=0.4.0->llama_index) (2025.4.26)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5,>=0.4.0->llama_index)\n",
      "  Downloading openai-1.86.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama_index) (4.13.3)\n",
      "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.2.3)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5,>=0.4.0->llama_index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.32-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama_index) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama_index) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama_index) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama_index) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama_index) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama_index) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.42->llama_index) (1.20.0)\n",
      "Collecting griffe (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama_index)\n",
      "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama_index) (3.1.6)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama_index) (4.3.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.6)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.42->llama_index) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.42->llama_index) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13,>=0.12.42->llama_index) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.42->llama_index) (0.14.0)\n",
      "Collecting llama-cloud-services>=0.6.32 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.32-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama_index) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.13,>=0.12.42->llama_index) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.13,>=0.12.42->llama_index) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.13,>=0.12.42->llama_index) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.13,>=0.12.42->llama_index) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.13,>=0.12.42->llama_index) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.13,>=0.12.42->llama_index) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.42->llama_index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.42->llama_index) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.42->llama_index) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.42->llama_index) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.42->llama_index) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.42->llama_index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.42->llama_index) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.42->llama_index) (3.26.1)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.32->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index) (1.1.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.42->llama_index) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama_index) (1.17.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama_index) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.42->llama_index) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.13,>=0.12.42->llama_index) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.13,>=0.12.42->llama_index) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->llama-index-core<0.13,>=0.12.42->llama_index) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->llama-index-core<0.13,>=0.12.42->llama_index) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->llama-index-core<0.13,>=0.12.42->llama_index) (2024.2.0)\n",
      "Downloading llama_index-0.12.42-py3-none-any.whl (7.1 kB)\n",
      "Downloading llama_index_agent_openai-0.4.11-py3-none-any.whl (14 kB)\n",
      "Downloading llama_index_cli-0.4.3-py3-none-any.whl (28 kB)\n",
      "Downloading llama_index_core-0.12.42-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.7.7-py3-none-any.whl (16 kB)\n",
      "Downloading llama_cloud-0.1.26-py3-none-any.whl (266 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai-0.4.5-py3-none-any.whl (25 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl (3.4 kB)\n",
      "Downloading llama_index_program_openai-0.3.2-py3-none-any.whl (6.1 kB)\n",
      "Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl (3.7 kB)\n",
      "Downloading llama_index_readers_file-0.4.9-py3-none-any.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading banks-2.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading llama_parse-0.6.32-py3-none-any.whl (4.9 kB)\n",
      "Downloading openai-1.86.0-py3-none-any.whl (730 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.3/730.3 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading llama_cloud_services-0.6.32-py3-none-any.whl (39 kB)\n",
      "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: striprtf, filetype, dirtyjson, griffe, openai, llama-cloud, banks, llama-index-core, llama-index-llms-openai, llama-index-agent-openai, llama-cloud-services, llama-parse, llama-index-program-openai, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-readers-file, llama-index-question-gen-openai, llama-index-multi-modal-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-cli, llama_index\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.70.0\n",
      "    Uninstalling openai-1.70.0:\n",
      "      Successfully uninstalled openai-1.70.0\n",
      "Successfully installed banks-2.1.2 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.7.3 llama-cloud-0.1.26 llama-cloud-services-0.6.32 llama-index-agent-openai-0.4.11 llama-index-cli-0.4.3 llama-index-core-0.12.42 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.7.7 llama-index-llms-openai-0.4.5 llama-index-multi-modal-llms-openai-0.5.1 llama-index-program-openai-0.3.2 llama-index-question-gen-openai-0.3.1 llama-index-readers-file-0.4.9 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.32 llama_index-0.12.42 openai-1.86.0 striprtf-0.0.26\n",
      "Found existing installation: pymongo 4.12.1\n",
      "Uninstalling pymongo-4.12.1:\n",
      "  Successfully uninstalled pymongo-4.12.1\n",
      "Collecting pymongo==3.13.0 (from pymongo[srv]==3.13.0)\n",
      "  Downloading pymongo-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo[srv]==3.13.0) (2.7.0)\n",
      "Downloading pymongo-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (518 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.1/518.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymongo\n",
      "Successfully installed pymongo-3.13.0\n",
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
      "Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: pyngrok\n",
      "Successfully installed pyngrok-7.2.11\n",
      "Collecting scipy==1.10.1\n",
      "  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scipy==1.10.1) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<1.27.0,>=1.19.5->scipy==1.10.1) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<1.27.0,>=1.19.5->scipy==1.10.1) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<1.27.0,>=1.19.5->scipy==1.10.1) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<1.27.0,>=1.19.5->scipy==1.10.1) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<1.27.0,>=1.19.5->scipy==1.10.1) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<1.27.0,>=1.19.5->scipy==1.10.1) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<1.27.0,>=1.19.5->scipy==1.10.1) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<1.27.0,>=1.19.5->scipy==1.10.1) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<1.27.0,>=1.19.5->scipy==1.10.1) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<1.27.0,>=1.19.5->scipy==1.10.1) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<1.27.0,>=1.19.5->scipy==1.10.1) (2024.2.0)\n",
      "Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.2\n",
      "    Uninstalling scipy-1.15.2:\n",
      "      Successfully uninstalled scipy-1.15.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\n",
      "kaggle-environments 1.16.11 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\n",
      "kaggle-environments 1.16.11 requires transformers>=4.33.1, but you have transformers 4.33.0 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "cvxpy 1.6.4 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scipy-1.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y huggingface_hub\n",
    "!pip install huggingface_hub==0.15.1\n",
    "!pip install pydantic-settings==2.0.3\n",
    "!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\n",
    "bitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.15\n",
    "!pip install pypdf llama_index\n",
    "!pip uninstall pymongo -y\n",
    "!pip install \"pymongo[srv]==3.13.0\"\n",
    "!pip install pyngrok\n",
    "\n",
    "import dns\n",
    "!pip install scipy==1.10.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:22:08.447906Z",
     "iopub.status.busy": "2025-06-14T16:22:08.447568Z",
     "iopub.status.idle": "2025-06-14T16:25:11.046677Z",
     "shell.execute_reply": "2025-06-14T16:25:11.045955Z",
     "shell.execute_reply.started": "2025-06-14T16:22:08.447876Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.41.1)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting torch<3,>=2.2 (from bitsandbytes)\n",
      "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.13.2)\n",
      "Collecting sympy>=1.13.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m920.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, bitsandbytes\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.0.0\n",
      "    Uninstalling triton-2.0.0:\n",
      "      Successfully uninstalled triton-2.0.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Uninstalling torch-2.0.1:\n",
      "      Successfully uninstalled torch-2.0.1\n",
      "  Attempting uninstall: bitsandbytes\n",
      "    Found existing installation: bitsandbytes 0.41.1\n",
      "    Uninstalling bitsandbytes-0.41.1:\n",
      "      Successfully uninstalled bitsandbytes-0.41.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.15.2 requires torch==2.0.1, but you have torch 2.7.1 which is incompatible.\n",
      "xformers 0.0.21 requires torch==2.0.1, but you have torch 2.7.1 which is incompatible.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
      "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.46.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.1 triton-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:25:11.048874Z",
     "iopub.status.busy": "2025-06-14T16:25:11.048632Z",
     "iopub.status.idle": "2025-06-14T16:25:22.657215Z",
     "shell.execute_reply": "2025-06-14T16:25:22.656611Z",
     "shell.execute_reply.started": "2025-06-14T16:25:11.048849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "    from torch import cuda, bfloat16\n",
    "    import torch\n",
    "    import transformers\n",
    "    from transformers import AutoTokenizer\n",
    "    from time import time\n",
    "    import chromadb\n",
    "    from chromadb.config import Settings\n",
    "    from langchain.llms import HuggingFacePipeline\n",
    "    from langchain.document_loaders import TextLoader\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.vectorstores import Chroma\n",
    "    \n",
    "    import json\n",
    "    from pymongo import MongoClient\n",
    "    import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:25:22.658266Z",
     "iopub.status.busy": "2025-06-14T16:25:22.657913Z",
     "iopub.status.idle": "2025-06-14T16:25:22.939153Z",
     "shell.execute_reply": "2025-06-14T16:25:22.938573Z",
     "shell.execute_reply.started": "2025-06-14T16:25:22.658248Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Model details\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"  # Example for LLaMA 2 7B Chat Model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Quantization Configuration\n",
    "# for gpu\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:25:22.940164Z",
     "iopub.status.busy": "2025-06-14T16:25:22.939923Z",
     "iopub.status.idle": "2025-06-14T16:25:30.384297Z",
     "shell.execute_reply": "2025-06-14T16:25:30.383462Z",
     "shell.execute_reply.started": "2025-06-14T16:25:22.940136Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting triton==2.1.0\n",
      "  Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton==2.1.0) (3.18.0)\n",
      "Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.3.1\n",
      "    Uninstalling triton-3.3.1:\n",
      "      Successfully uninstalled triton-3.3.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.7.1 requires triton==3.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 2.1.0 which is incompatible.\n",
      "xformers 0.0.21 requires torch==2.0.1, but you have torch 2.7.1 which is incompatible.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
      "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed triton-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install triton==2.1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:25:30.385565Z",
     "iopub.status.busy": "2025-06-14T16:25:30.385315Z",
     "iopub.status.idle": "2025-06-14T16:27:13.685203Z",
     "shell.execute_reply": "2025-06-14T16:27:13.684322Z",
     "shell.execute_reply.started": "2025-06-14T16:25:30.385536Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f5cc286fd8450a9d3e89370d1ecb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "2025-06-14 16:25:37.691938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749918338.163257      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749918338.277976      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3daff9757340c7ba1a6ae0a3ac3234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5eb3920bc04ea1885be9e445c20fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f37748c71394e5ab96d211858e253be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dade6739df7406d9f219e972a16e7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39fa15d11684da3a62c85ffe71c0287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839fdaa75acf49d8a51e1e5a4665dfb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9cc58ce1484bafbf2630bb75008832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4076fe9f5f4cf6aee49861b11c62f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bddafb9e04469f85dc23ad97536259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116ec2696e764eb7862c0b8a4d752e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare model, tokenizer: 103.294 sec.\n"
     ]
    }
   ],
   "source": [
    "# Loading Model\n",
    "\n",
    "time_1 = time()\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    ")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "time_2 = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setting Up Pipelines**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:27:13.687099Z",
     "iopub.status.busy": "2025-06-14T16:27:13.686216Z",
     "iopub.status.idle": "2025-06-14T16:27:17.644593Z",
     "shell.execute_reply": "2025-06-14T16:27:17.643911Z",
     "shell.execute_reply.started": "2025-06-14T16:27:13.687074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare pipeline: 3.953 sec.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preparing Pipeline\n",
    "\n",
    "time_1 = time()\n",
    "query_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",)\n",
    "time_2 = time()\n",
    "print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:27:17.645775Z",
     "iopub.status.busy": "2025-06-14T16:27:17.645452Z",
     "iopub.status.idle": "2025-06-14T16:27:17.650583Z",
     "shell.execute_reply": "2025-06-14T16:27:17.650019Z",
     "shell.execute_reply.started": "2025-06-14T16:27:17.645743Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_model(tokenizer, pipeline, prompt_to_test):\n",
    "    \"\"\"\n",
    "    Perform a query\n",
    "    print the result\n",
    "    Args:\n",
    "        tokenizer: the tokenizer\n",
    "        pipeline: the pipeline\n",
    "        prompt_to_test: the prompt\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"\n",
    "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
    "    time_1 = time()\n",
    "    sequences = pipeline(\n",
    "        prompt_to_test,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,)\n",
    "    time_2 = time()\n",
    "    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:27:17.654437Z",
     "iopub.status.busy": "2025-06-14T16:27:17.654019Z",
     "iopub.status.idle": "2025-06-14T16:27:29.105519Z",
     "shell.execute_reply": "2025-06-14T16:27:29.104677Z",
     "shell.execute_reply.started": "2025-06-14T16:27:17.654409Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test inference: 11.426 sec.\n",
      "Result: Please explain what are Newton's Law of Motion. Keep it in 100 words.\n",
      "\n",
      "Newton's Law of Motion states that an object at rest will remain at rest, and an object in motion will continue to move with a constant velocity, unless acted upon by an external force. The law is often summarized as \"F = ma,\" where F is the net force acting on an object, m is its mass, and a is its acceleration.\n"
     ]
    }
   ],
   "source": [
    "# Testing \n",
    "\n",
    "test_model(tokenizer,\n",
    "           query_pipeline,\n",
    "           \"Please explain what are Newton's Law of Motion. Keep it in 100 words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:27:29.106699Z",
     "iopub.status.busy": "2025-06-14T16:27:29.106348Z",
     "iopub.status.idle": "2025-06-14T16:27:38.357545Z",
     "shell.execute_reply": "2025-06-14T16:27:38.356788Z",
     "shell.execute_reply.started": "2025-06-14T16:27:29.106671Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNewton's Law of Motion is a fundamental principle in physics that describes how an object moves or responds to forces acting upon it. It states that an object at rest will remain at rest, and an object in motion will continue to move with a constant velocity, unless acted upon by an external force. The law also relates the force applied to an object to its resulting acceleration, or change in velocity.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "# checking again that everything is working fine\n",
    "llm(prompt=\"Please explain what are Newton's Law of Motion. Keep it in 100 words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setting-Up Embeddings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:27:38.358793Z",
     "iopub.status.busy": "2025-06-14T16:27:38.358410Z",
     "iopub.status.idle": "2025-06-14T16:27:56.359295Z",
     "shell.execute_reply": "2025-06-14T16:27:56.358571Z",
     "shell.execute_reply.started": "2025-06-14T16:27:38.358762Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6897608e920a40fdb1718e89d3d580f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading .gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572ddb01fcd141bf851c537b4fcf685f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbb77890eab4668ac261e6cecef5ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/90.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27375443d859435584e4081c70a55228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da89690338c643b4bfbe494693d188bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc98753ed9c43c7a663154de70695af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd5cf9ef9cc47d19852e6214460b2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f162a2fb03f4fbeb656f976189bd6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110ad61426ff4bd6b481333fdf7acda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01d769f25b04a3a82b66367d5bc6720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98fda8293e664703a5c80f4bfd139e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1cee660dae4c5eb72a9cf9493d5b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67951ef34e8146a6a212e236bdcdf17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Loading an Embedding Model\n",
    "\n",
    "\n",
    "model_name = \"BAAI/bge-large-en\"  # ✅ Better retrieval model\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, \n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "print(\"Embedding model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:27:56.360370Z",
     "iopub.status.busy": "2025-06-14T16:27:56.360120Z",
     "iopub.status.idle": "2025-06-14T16:28:16.316525Z",
     "shell.execute_reply": "2025-06-14T16:28:16.315668Z",
     "shell.execute_reply.started": "2025-06-14T16:27:56.360351Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting cohere\n",
      "  Downloading cohere-5.15.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
      "  Downloading fastavro-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.28.1)\n",
      "Collecting httpx-sse==0.4.0 (from cohere)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.11.4)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.33.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.3)\n",
      "Collecting tokenizers<1,>=0.15 (from cohere)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
      "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (4.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->cohere) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.4.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers<1,>=0.15->cohere)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere)\n",
      "  Downloading hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
      "Downloading cohere-5.15.0-py3-none-any.whl (259 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading fastavro-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
      "Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.8/514.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: types-requests, httpx-sse, hf-xet, fastavro, huggingface-hub, tokenizers, cohere\n",
      "  Attempting uninstall: hf-xet\n",
      "    Found existing installation: hf-xet 1.1.0\n",
      "    Uninstalling hf-xet-1.1.0:\n",
      "      Successfully uninstalled hf-xet-1.1.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.15.1\n",
      "    Uninstalling huggingface-hub-0.15.1:\n",
      "      Successfully uninstalled huggingface-hub-0.15.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.33.0 requires tokenizers!=0.11.3,<0.14,>=0.11.1, but you have tokenizers 0.21.1 which is incompatible.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\n",
      "kaggle-environments 1.16.11 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\n",
      "kaggle-environments 1.16.11 requires transformers>=4.33.1, but you have transformers 4.33.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cohere-5.15.0 fastavro-1.11.1 hf-xet-1.1.3 httpx-sse-0.4.0 huggingface-hub-0.33.0 tokenizers-0.21.1 types-requests-2.32.4.20250611\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 🔹 Extra Library for reranking \n",
    "\n",
    "pip install cohere\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Chatbot Code**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T18:13:31.471319Z",
     "iopub.status.busy": "2025-06-14T18:13:31.470824Z",
     "iopub.status.idle": "2025-06-14T18:24:29.449243Z",
     "shell.execute_reply": "2025-06-14T18:24:29.448243Z",
     "shell.execute_reply.started": "2025-06-14T18:13:31.471296Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Ngrok tunnel established: NgrokTunnel: \"https://49d8-34-136-197-54.ngrok-free.app\" -> \"http://localhost:5000\"\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f494d036f5b24c20b52e8b2b1278674e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='without difficulty know it, Thanks a lot.\\nThere is definately a great deal to find out about this issue. I really like all the points you made\\nsays:\\nAugust 21, 2023\\nThere is definately a great deal to find out about this issue.\\nI really like all the points you made.\\nI was more than happy to discover this page. I want to to  thank you for ones time for this particul\\nsays:\\nAugust 22, 2023\\nI was more than happy to discover this page.\\nI want to to thank you for ones time for this particularly wonderful read!!\\nI definitely really liked every bit of it and i also have you bookmarked to see new \\nstuff in your site.\\nHello, I enjoy reading through your article post. I wanted to  write a little comment to support you\\nsays:\\nOctober 1, 2023\\nHello, I enjoy reading through your article post. I wanted to write a little comment to support \\nyou.\\nThanks for finally talking about >Online store automation\\nsays:\\nDecember 2, 2023\\nThanks for finally talking about >Online store automation', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='Imagine the situation. You have a mid-sized store and a customer called and placed an order. But you don�t know if the product is in stock. You need to tell the client that you will clarify the information and call back. Next, you need to call the warehouse, ask the employee about the availability of the goods. He will go to watch it, and after a couple of minutes he will call back to say that he is. Now you need to call the client and report it. Everything takes 4 to 15 minutes. And if there is no product? You call the client, talk about it, after which he may inquire about another product. The chain will have to be repeated again. It�s even worse if he just says he�ll order from another store.\\nAutomation of product price calculation and invoicing', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='A visitor enters the site, examines the products, site pages, reads information about the company, delivery, and payment terms.\\nIf the user has not received valuable information for himself, he immediately leaves the site and goes to search further. It is irretrievably lost to the store. If something interested him, there is a chance that he will leave at least some of his data. Phone, mail, name.\\nIf the visitor is determined, he adds the product to the cart. At this stage, most of the visitors are lost. The fact is that the reason for adding to the cart can be different: a comparison of prices, a total count of goods, just such a desire appeared. In a classic sales funnel, it is extremely difficult to understand the reason.\\nUnits go even further and place an order. Of course, if they do not face difficulties in registration, objections, or other factors that can turn them away from the purchase.', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='says:\\nJuly 12, 2023\\nI love it when individuals come together and share opinions.\\nGreat website, continue the good work!\\nHi Dear, are you genuinely visiting this website  daily, if so then you will absolutely obtain nice\\nsays:\\nAugust 12, 2023\\nHi Dear, are you genuinely visiting this website daily,\\nif so then you will absolutely obtain nice experience.\\nHey there! This is my first visit to your blog! We are a  team of volunteers and starting a new init\\nsays:\\nAugust 14, 2023\\nHey there! This is my first visit to your blog! We are a \\nteam of volunteers and starting a new initiative in a community in the same niche.\\nYour blog provided us valuable information to work on. \\nYou have done a marvellous job!\\nYour means of explaining everything in this article is actually  pleasant, every one be able to with\\nsays:\\nAugust 16, 2023\\nYour means of explaining everything in this article is actually pleasant, every one be able to \\nwithout difficulty know it, Thanks a lot.', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='says:\\nNovember 29, 2022\\nVery nice, was looking for that content\\nthanks\\nI couⅾ not refrain from commenting. Exceptionally well  ᴡritten!\\nsays:\\nDecember 14, 2022\\nӀ could not refrain from commenting. Exceptionally weⅼl \\nwritten!\\nLeave a Comment\\nYour email address will not be published.\\nRequired fields are marked\\n*\\nComment\\n*\\nName\\n*\\nEmail\\n*\\nWebsite\\nProviding software development services and support to clients all over the world.\\n+923095930315\\n+44 7466 109725\\nPakistan, PK\\nWah, Pakistan\\nMinaar road, Lala Rukh St, Wah Cantt, 47040\\nUnited Kingdom\\nBradford, UK\\n27 Hollybank Gardens, Bradford BD7 4QR, UK\\nInfo@exytex.com\\nDubai, AE\\nDubai, UAE\\nFrance Cluster, Pavilion Mall, P-05, International City\\n+971 52201 9994\\nGet In Touch\\nRegistered By PSEB\\nCOPYRIGHT @2024\\nEXYTEX Technologies\\nⓇ | All rights reserved.\\nHome\\nBlog\\nPortfolio\\nTeam\\nContact', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='Build your software features that will benefit from Fernando�s knowledge of the industry, making him feel even more proud of what he is capable of doing.\\nAlso prepare ways to deal with the problem he has with his team, enabling efficient team management in your system!\\nYour service and your commercial also benefit! After all, when talking to customers or prospects they don�t know, they won�t be talking to a faceless stat, but to Fernando!\\nThis makes the service empathetic, more humanized, and goes straight to your customer�s pain, greatly improving customer satisfaction and also the sales rate!\\nPersonas and Inbound Marketing for software company!\\nPlus you still have marketing! Before getting into your specifics, we can say with confidence: The use of personas makes your marketing attention be better directed.\\nAs a result, you have increased conversion rates and avoid contacting customers who will not be interested in the solutions you can offer.', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='farhan\\nsays:\\nNovember 6, 2022\\nReally very nice information on this site. Thanks for sharing this nice information. I am very happy to read this post and I really appreciate it. Thanks for sharing this useful information with us it’s great info keep sharing.\\nplease visit :\\nhttps://fetchlogos.com/\\nVery good article. I will be experiencing a few of these issues  as well..\\nsays:\\nDecember 4, 2022\\nVery good article. I will be experiencing a few of these issues as well..\\nLeave a Comment\\nYour email address will not be published.\\nRequired fields are marked\\n*\\nComment\\n*\\nName\\n*\\nEmail\\n*\\nWebsite\\nProviding software development services and support to clients all over the world.\\n+923095930315\\n+44 7466 109725\\nPakistan, PK\\nWah, Pakistan\\nMinaar road, Lala Rukh St, Wah Cantt, 47040\\nUnited Kingdom\\nBradford, UK\\n27 Hollybank Gardens, Bradford BD7 4QR, UK\\nInfo@exytex.com\\nDubai, AE\\nDubai, UAE\\nFrance Cluster, Pavilion Mall, P-05, International City\\n+971 52201 9994\\nGet In Touch\\nRegistered By PSEB', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'})]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35192bc9980c4ccbb9b2ca1356de5ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hello! Thank you for reaching out to us. At Exytex Technologies, we specialize in providing software development services and support to clients worldwide. Our team of experienced professionals is dedicated to delivering high-quality solutions that meet your unique needs and requirements. We have a proven track record of success and a passion for staying at the forefront of the latest technologies.\n",
      "\n",
      "Fallback Answer: I apologize, but I don't have access to specific information about Exytex Technologies' services and company details. However, I can provide general information on software development and the technology industry. Please let me know if there's anything else I can help you with.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ee19e7eb5140a692c592cf2799e039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='without difficulty know it, Thanks a lot.\\nThere is definately a great deal to find out about this issue. I really like all the points you made\\nsays:\\nAugust 21, 2023\\nThere is definately a great deal to find out about this issue.\\nI really like all the points you made.\\nI was more than happy to discover this page. I want to to  thank you for ones time for this particul\\nsays:\\nAugust 22, 2023\\nI was more than happy to discover this page.\\nI want to to thank you for ones time for this particularly wonderful read!!\\nI definitely really liked every bit of it and i also have you bookmarked to see new \\nstuff in your site.\\nHello, I enjoy reading through your article post. I wanted to  write a little comment to support you\\nsays:\\nOctober 1, 2023\\nHello, I enjoy reading through your article post. I wanted to write a little comment to support \\nyou.\\nThanks for finally talking about >Online store automation\\nsays:\\nDecember 2, 2023\\nThanks for finally talking about >Online store automation', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='Imagine the situation. You have a mid-sized store and a customer called and placed an order. But you don�t know if the product is in stock. You need to tell the client that you will clarify the information and call back. Next, you need to call the warehouse, ask the employee about the availability of the goods. He will go to watch it, and after a couple of minutes he will call back to say that he is. Now you need to call the client and report it. Everything takes 4 to 15 minutes. And if there is no product? You call the client, talk about it, after which he may inquire about another product. The chain will have to be repeated again. It�s even worse if he just says he�ll order from another store.\\nAutomation of product price calculation and invoicing', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='A visitor enters the site, examines the products, site pages, reads information about the company, delivery, and payment terms.\\nIf the user has not received valuable information for himself, he immediately leaves the site and goes to search further. It is irretrievably lost to the store. If something interested him, there is a chance that he will leave at least some of his data. Phone, mail, name.\\nIf the visitor is determined, he adds the product to the cart. At this stage, most of the visitors are lost. The fact is that the reason for adding to the cart can be different: a comparison of prices, a total count of goods, just such a desire appeared. In a classic sales funnel, it is extremely difficult to understand the reason.\\nUnits go even further and place an order. Of course, if they do not face difficulties in registration, objections, or other factors that can turn them away from the purchase.', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='says:\\nJuly 12, 2023\\nI love it when individuals come together and share opinions.\\nGreat website, continue the good work!\\nHi Dear, are you genuinely visiting this website  daily, if so then you will absolutely obtain nice\\nsays:\\nAugust 12, 2023\\nHi Dear, are you genuinely visiting this website daily,\\nif so then you will absolutely obtain nice experience.\\nHey there! This is my first visit to your blog! We are a  team of volunteers and starting a new init\\nsays:\\nAugust 14, 2023\\nHey there! This is my first visit to your blog! We are a \\nteam of volunteers and starting a new initiative in a community in the same niche.\\nYour blog provided us valuable information to work on. \\nYou have done a marvellous job!\\nYour means of explaining everything in this article is actually  pleasant, every one be able to with\\nsays:\\nAugust 16, 2023\\nYour means of explaining everything in this article is actually pleasant, every one be able to \\nwithout difficulty know it, Thanks a lot.', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='says:\\nNovember 29, 2022\\nVery nice, was looking for that content\\nthanks\\nI couⅾ not refrain from commenting. Exceptionally well  ᴡritten!\\nsays:\\nDecember 14, 2022\\nӀ could not refrain from commenting. Exceptionally weⅼl \\nwritten!\\nLeave a Comment\\nYour email address will not be published.\\nRequired fields are marked\\n*\\nComment\\n*\\nName\\n*\\nEmail\\n*\\nWebsite\\nProviding software development services and support to clients all over the world.\\n+923095930315\\n+44 7466 109725\\nPakistan, PK\\nWah, Pakistan\\nMinaar road, Lala Rukh St, Wah Cantt, 47040\\nUnited Kingdom\\nBradford, UK\\n27 Hollybank Gardens, Bradford BD7 4QR, UK\\nInfo@exytex.com\\nDubai, AE\\nDubai, UAE\\nFrance Cluster, Pavilion Mall, P-05, International City\\n+971 52201 9994\\nGet In Touch\\nRegistered By PSEB\\nCOPYRIGHT @2024\\nEXYTEX Technologies\\nⓇ | All rights reserved.\\nHome\\nBlog\\nPortfolio\\nTeam\\nContact', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='Build your software features that will benefit from Fernando�s knowledge of the industry, making him feel even more proud of what he is capable of doing.\\nAlso prepare ways to deal with the problem he has with his team, enabling efficient team management in your system!\\nYour service and your commercial also benefit! After all, when talking to customers or prospects they don�t know, they won�t be talking to a faceless stat, but to Fernando!\\nThis makes the service empathetic, more humanized, and goes straight to your customer�s pain, greatly improving customer satisfaction and also the sales rate!\\nPersonas and Inbound Marketing for software company!\\nPlus you still have marketing! Before getting into your specifics, we can say with confidence: The use of personas makes your marketing attention be better directed.\\nAs a result, you have increased conversion rates and avoid contacting customers who will not be interested in the solutions you can offer.', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='farhan\\nsays:\\nNovember 6, 2022\\nReally very nice information on this site. Thanks for sharing this nice information. I am very happy to read this post and I really appreciate it. Thanks for sharing this useful information with us it’s great info keep sharing.\\nplease visit :\\nhttps://fetchlogos.com/\\nVery good article. I will be experiencing a few of these issues  as well..\\nsays:\\nDecember 4, 2022\\nVery good article. I will be experiencing a few of these issues as well..\\nLeave a Comment\\nYour email address will not be published.\\nRequired fields are marked\\n*\\nComment\\n*\\nName\\n*\\nEmail\\n*\\nWebsite\\nProviding software development services and support to clients all over the world.\\n+923095930315\\n+44 7466 109725\\nPakistan, PK\\nWah, Pakistan\\nMinaar road, Lala Rukh St, Wah Cantt, 47040\\nUnited Kingdom\\nBradford, UK\\n27 Hollybank Gardens, Bradford BD7 4QR, UK\\nInfo@exytex.com\\nDubai, AE\\nDubai, UAE\\nFrance Cluster, Pavilion Mall, P-05, International City\\n+971 52201 9994\\nGet In Touch\\nRegistered By PSEB', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'})]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb359cb0885b46ae86c33a7fa01f8835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Thank you for reaching out! Exytex Technologies is a software development company that specializes in providing high-quality services to clients all over the world. We have a team of skilled professionals who are dedicated to delivering exceptional results and exceeding customer expectations. Our services include custom software development, web development, mobile app development, and more. We take pride in our ability to understand the unique needs of each client and provide tailored solutions that meet their requirements.\n",
      "\n",
      "Required fields are marked\n",
      "*\n",
      "Comment\n",
      "*\n",
      "Name\n",
      "*\n",
      "Email\n",
      "*\n",
      "Website\n",
      "Providing software development services and support to clients all over the world.\n",
      "+923095930315\n",
      "+44 7466 109725\n",
      "Pakistan, PK\n",
      "Wah, Pakistan\n",
      "Minaar road, Lala Rukh St, Wah Cantt, 47040\n",
      "United Kingdom\n",
      "Bradford, UK\n",
      "27 Hollybank Gardens, Bradford BD7 4QR, UK\n",
      "Info@exytex.com\n",
      "Dubai, AE\n",
      "Dubai, UAE\n",
      "France Cluster, Pavilion Mall, P-05, International City\n",
      "+971 52201 9994\n",
      "Get In Touch\n",
      "Registered By PSEB\n",
      "\n",
      "Please provide the user's query, and I will do my best to answer it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a849c712264bf5a8473caf8b916c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='without difficulty know it, Thanks a lot.\\nThere is definately a great deal to find out about this issue. I really like all the points you made\\nsays:\\nAugust 21, 2023\\nThere is definately a great deal to find out about this issue.\\nI really like all the points you made.\\nI was more than happy to discover this page. I want to to  thank you for ones time for this particul\\nsays:\\nAugust 22, 2023\\nI was more than happy to discover this page.\\nI want to to thank you for ones time for this particularly wonderful read!!\\nI definitely really liked every bit of it and i also have you bookmarked to see new \\nstuff in your site.\\nHello, I enjoy reading through your article post. I wanted to  write a little comment to support you\\nsays:\\nOctober 1, 2023\\nHello, I enjoy reading through your article post. I wanted to write a little comment to support \\nyou.\\nThanks for finally talking about >Online store automation\\nsays:\\nDecember 2, 2023\\nThanks for finally talking about >Online store automation', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='A visitor enters the site, examines the products, site pages, reads information about the company, delivery, and payment terms.\\nIf the user has not received valuable information for himself, he immediately leaves the site and goes to search further. It is irretrievably lost to the store. If something interested him, there is a chance that he will leave at least some of his data. Phone, mail, name.\\nIf the visitor is determined, he adds the product to the cart. At this stage, most of the visitors are lost. The fact is that the reason for adding to the cart can be different: a comparison of prices, a total count of goods, just such a desire appeared. In a classic sales funnel, it is extremely difficult to understand the reason.\\nUnits go even further and place an order. Of course, if they do not face difficulties in registration, objections, or other factors that can turn them away from the purchase.', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='Imagine the situation. You have a mid-sized store and a customer called and placed an order. But you don�t know if the product is in stock. You need to tell the client that you will clarify the information and call back. Next, you need to call the warehouse, ask the employee about the availability of the goods. He will go to watch it, and after a couple of minutes he will call back to say that he is. Now you need to call the client and report it. Everything takes 4 to 15 minutes. And if there is no product? You call the client, talk about it, after which he may inquire about another product. The chain will have to be repeated again. It�s even worse if he just says he�ll order from another store.\\nAutomation of product price calculation and invoicing', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='says:\\nNovember 29, 2022\\nVery nice, was looking for that content\\nthanks\\nI couⅾ not refrain from commenting. Exceptionally well  ᴡritten!\\nsays:\\nDecember 14, 2022\\nӀ could not refrain from commenting. Exceptionally weⅼl \\nwritten!\\nLeave a Comment\\nYour email address will not be published.\\nRequired fields are marked\\n*\\nComment\\n*\\nName\\n*\\nEmail\\n*\\nWebsite\\nProviding software development services and support to clients all over the world.\\n+923095930315\\n+44 7466 109725\\nPakistan, PK\\nWah, Pakistan\\nMinaar road, Lala Rukh St, Wah Cantt, 47040\\nUnited Kingdom\\nBradford, UK\\n27 Hollybank Gardens, Bradford BD7 4QR, UK\\nInfo@exytex.com\\nDubai, AE\\nDubai, UAE\\nFrance Cluster, Pavilion Mall, P-05, International City\\n+971 52201 9994\\nGet In Touch\\nRegistered By PSEB\\nCOPYRIGHT @2024\\nEXYTEX Technologies\\nⓇ | All rights reserved.\\nHome\\nBlog\\nPortfolio\\nTeam\\nContact', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='says:\\nJuly 12, 2023\\nI love it when individuals come together and share opinions.\\nGreat website, continue the good work!\\nHi Dear, are you genuinely visiting this website  daily, if so then you will absolutely obtain nice\\nsays:\\nAugust 12, 2023\\nHi Dear, are you genuinely visiting this website daily,\\nif so then you will absolutely obtain nice experience.\\nHey there! This is my first visit to your blog! We are a  team of volunteers and starting a new init\\nsays:\\nAugust 14, 2023\\nHey there! This is my first visit to your blog! We are a \\nteam of volunteers and starting a new initiative in a community in the same niche.\\nYour blog provided us valuable information to work on. \\nYou have done a marvellous job!\\nYour means of explaining everything in this article is actually  pleasant, every one be able to with\\nsays:\\nAugust 16, 2023\\nYour means of explaining everything in this article is actually pleasant, every one be able to \\nwithout difficulty know it, Thanks a lot.', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='farhan\\nsays:\\nNovember 6, 2022\\nReally very nice information on this site. Thanks for sharing this nice information. I am very happy to read this post and I really appreciate it. Thanks for sharing this useful information with us it’s great info keep sharing.\\nplease visit :\\nhttps://fetchlogos.com/\\nVery good article. I will be experiencing a few of these issues  as well..\\nsays:\\nDecember 4, 2022\\nVery good article. I will be experiencing a few of these issues as well..\\nLeave a Comment\\nYour email address will not be published.\\nRequired fields are marked\\n*\\nComment\\n*\\nName\\n*\\nEmail\\n*\\nWebsite\\nProviding software development services and support to clients all over the world.\\n+923095930315\\n+44 7466 109725\\nPakistan, PK\\nWah, Pakistan\\nMinaar road, Lala Rukh St, Wah Cantt, 47040\\nUnited Kingdom\\nBradford, UK\\n27 Hollybank Gardens, Bradford BD7 4QR, UK\\nInfo@exytex.com\\nDubai, AE\\nDubai, UAE\\nFrance Cluster, Pavilion Mall, P-05, International City\\n+971 52201 9994\\nGet In Touch\\nRegistered By PSEB', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'}), Document(page_content='Build your software features that will benefit from Fernando�s knowledge of the industry, making him feel even more proud of what he is capable of doing.\\nAlso prepare ways to deal with the problem he has with his team, enabling efficient team management in your system!\\nYour service and your commercial also benefit! After all, when talking to customers or prospects they don�t know, they won�t be talking to a faceless stat, but to Fernando!\\nThis makes the service empathetic, more humanized, and goes straight to your customer�s pain, greatly improving customer satisfaction and also the sales rate!\\nPersonas and Inbound Marketing for software company!\\nPlus you still have marketing! Before getting into your specifics, we can say with confidence: The use of personas makes your marketing attention be better directed.\\nAs a result, you have increased conversion rates and avoid contacting customers who will not be interested in the solutions you can offer.', metadata={'source': './939166a50e50996a4137ebf02de4a305b93d56204dc30af10de0389795e14ba1.txt'})]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d1426dc641425686c81a262776ee00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Thank you for reaching out! Our company, Exytex Technologies, is a software development services provider with a team of experienced professionals. We offer a wide range of services, including product development, custom software solutions, and ongoing maintenance and support. Our goal is to help businesses like yours succeed through innovative technology solutions. Please let me know if you have any specific questions or if there's anything else I can help you with!\n"
     ]
    }
   ],
   "source": [
    "import cohere  \n",
    "from flask import Flask, request, jsonify\n",
    "from pyngrok import ngrok, conf\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "\n",
    "# 🔹 Set your NGROK_AUTH_TOKEN here\n",
    "NGROK_AUTH_TOKEN = \"\"  # ⬅️ Replace with your actual token\n",
    "conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
    "\n",
    "# Initialize Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# MongoDB Connection\n",
    "client = MongoClient(\"\")\n",
    "db = client[\"\"]\n",
    "collection = db[\"\"]\n",
    "\n",
    "# ChromaDB Setup\n",
    "chroma_path = \"./chroma_db\"\n",
    "os.makedirs(chroma_path, exist_ok=True)  \n",
    "\n",
    "# Initialize persistent ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=chroma_path)\n",
    "co = cohere.Client(\"\")  # ✅ Use your API key\n",
    "\n",
    "\n",
    "\n",
    "def strip_extra_conversation(response_text):\n",
    "    \"\"\"\n",
    "    Preserves the first Bot or 🔹 Bot response while stripping any subsequent content.\n",
    "    \"\"\"\n",
    "    # Identify the markers indicating Bot responses\n",
    "    strip_markers = [\"\\nBot:\", \"\\n🔹 Bot:\"]\n",
    "    \n",
    "    # Find the earliest occurrence of either \"Bot:\" or \"🔹 Bot:\"\n",
    "    first_marker_index = None\n",
    "    for marker in strip_markers:\n",
    "        if marker in response_text:\n",
    "            marker_index = response_text.find(marker)\n",
    "            if first_marker_index is None or marker_index < first_marker_index:\n",
    "                first_marker_index = marker_index\n",
    "\n",
    "    # If a Bot marker exists, keep everything before it (including the first Bot response)\n",
    "    if first_marker_index is not None:\n",
    "        response_text = response_text[:first_marker_index + len(\"🔹 Bot:\")].strip()\n",
    "\n",
    "    return response_text\n",
    "\n",
    "\n",
    "# 🛠 Create separate Chroma collections per key\n",
    "def get_chroma_collection(key):\n",
    "    return Chroma(\n",
    "        persist_directory=f\"./chroma_db/{key}\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "\n",
    "# Function to fetch data from MongoDB\n",
    "def fetch_from_mongodb(key):\n",
    "    \"\"\"Fetches data from MongoDB using the key.\"\"\"\n",
    "    data = collection.find_one({\"key\": key})\n",
    "    return data[\"text_data\"] if data else None\n",
    "\n",
    "\n",
    "def setup_chromadb_for_website(website_name, documents, embeddings):\n",
    "    \"\"\"\n",
    "    Initialize and persist ChromaDB for a specific website.\n",
    "    \"\"\"\n",
    "    directory = f\"chroma_db/{website_name.replace(' ', '_')}\"\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=directory\n",
    "    )\n",
    "    return vectordb\n",
    "\n",
    "def upload_to_chromadb(key, text_content, embeddings):\n",
    "    \"\"\"\n",
    "    Uploads data to ChromaDB after retrieving from MongoDB.\n",
    "    \"\"\"\n",
    "    print(f\"📤 Uploading data to ChromaDB for key: {key}...\")\n",
    "\n",
    "    # Load and split documents\n",
    "    temp_file_path = f\"./{key}.txt\"\n",
    "    with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text_content)\n",
    "\n",
    "    loader = TextLoader(temp_file_path, encoding=\"utf8\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Setup ChromaDB for the website\n",
    "    vectordb = setup_chromadb_for_website(key, all_splits, embeddings)\n",
    "    os.remove(temp_file_path)  # Cleanup temp file\n",
    "\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "def get_chroma_collection(key):\n",
    "    \"\"\"Returns an existing ChromaDB collection or None if missing.\"\"\"\n",
    "    try:\n",
    "        collection_path = f\"./chroma_db/{key}\"\n",
    "        if not os.path.exists(collection_path):\n",
    "            print(f\"🚨 ChromaDB collection not found for key: {key}\")\n",
    "            return None  # No collection exists yet\n",
    "\n",
    "        return Chroma(\n",
    "            persist_directory=collection_path,\n",
    "            embedding_function=embeddings  # Ensure embeddings are initialized\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error accessing ChromaDB for key {key}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "@app.route(\"/query\", methods=[\"POST\"])\n",
    "def process_query():\n",
    "    \"\"\"Handles a user query with conversational context.\"\"\"\n",
    "    data = request.get_json()\n",
    "    query = data.get(\"query\")\n",
    "    key = data.get(\"key\")\n",
    "    user_id = data.get(\"user_id\")  # Ensure client sends a unique user ID\n",
    "\n",
    "    if not query or not key or not user_id:\n",
    "        return jsonify({\"error\": \"Missing query, key, or user ID\"}), 400\n",
    "\n",
    "    if query.lower() == \"resethistory\":\n",
    "        # Delete all conversation history for the given user_id\n",
    "        collection2 = db[\"history\"]\n",
    "        result = collection2.delete_many({\"user_id\": user_id})\n",
    "\n",
    "        return jsonify({\n",
    "            \"response\": f\"Conversation history has been reset for user_id: {user_id}.\",\n",
    "            \"deleted_count\": result.deleted_count\n",
    "        })\n",
    "    \n",
    "    # 🔹 Retrieve previous conversation context\n",
    "    collection2 = db[\"\"]\n",
    "    \n",
    "    conversation = collection2.find_one({\"user_id\": user_id, \"session_id\": key})\n",
    "    history = conversation[\"messages\"] if conversation else []\n",
    "\n",
    "    # Format conversation history for better context\n",
    "    formatted_history = \"\\n\".join([f\"{msg['role']}: {msg['text']}\" for msg in history])\n",
    "    context_query = f\"You are a friendly, professional, and knowledgeable customer support agent at the company retrieved from document. Your goal is to provide accurate, concise, and helpful responses. Answer the user's query based on the available data. Avoid repeating fallback statements unless you are certain the data is unavailable. Remain polite and approachable.\\n{formatted_history}\\nUser: {query}\\nBot:\"\n",
    "\n",
    "    # 🔍 Step 1: Try retrieving from ChromaDB\n",
    "    vectordb = get_chroma_collection(key)\n",
    "    if vectordb:  \n",
    "        retriever = vectordb.as_retriever(search_kwargs={\"k\": 7})\n",
    "\n",
    " \n",
    "        qa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        docs = qa.retriever.get_relevant_documents(context_query)\n",
    "        print(docs)\n",
    "\n",
    "\n",
    "        # 🔹 Step 3: Apply Reranking\n",
    "        rerank_input = [doc.page_content for doc in docs]  # ✅ Extract text for reranking\n",
    "        reranked_docs = co.rerank(model=\"rerank-english-v3.0\", query=query, documents=rerank_input, top_n=10)\n",
    "        \n",
    "        # Keep only the top reranked docs\n",
    "        docs = [docs[item.index] for item in reranked_docs.results] \n",
    "\n",
    "        \n",
    "        if docs:\n",
    "            response = qa.run(context_query)\n",
    "            print(response)\n",
    "\n",
    "            response = strip_extra_conversation(response)\n",
    "\n",
    "            # Store new conversation data\n",
    "            collection2.update_one(\n",
    "                {\"user_id\": user_id, \"session_id\": key},\n",
    "                {\"$push\": {\"messages\": {\"role\": \"user\", \"text\": query}}},\n",
    "                upsert=True\n",
    "            )\n",
    "            collection2.update_one(\n",
    "                {\"user_id\": user_id, \"session_id\": key},\n",
    "                {\"$push\": {\"messages\": {\"role\": \"bot\", \"text\": response}}},\n",
    "                upsert=True\n",
    "            )\n",
    "\n",
    "            return jsonify({\"response\": response})\n",
    "\n",
    "    # 🔄 Step 2: Fallback to MongoDB if ChromaDB has no data\n",
    "    mongo_data = fetch_from_mongodb(key)\n",
    "    if mongo_data:\n",
    "        upload_to_chromadb(key, mongo_data, embeddings)  # Store in ChromaDB\n",
    "        vectordb = get_chroma_collection(key)  \n",
    "\n",
    "        if vectordb:\n",
    "            retriever = vectordb.as_retriever(search_kwargs={\"k\": 7})\n",
    "\n",
    "            qa = RetrievalQA.from_chain_type(\n",
    "                llm=llm,\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=retriever,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            docs = qa.retriever.get_relevant_documents(context_query)\n",
    "            print(docs)\n",
    "            rerank_input = [doc.page_content for doc in docs]  # ✅ Extract text for reranking\n",
    "            reranked_docs = co.rerank(model=\"rerank-english-v3.0\", query=query, documents=rerank_input, top_n=10)\n",
    "        \n",
    "        # Keep only the top reranked docs\n",
    "            docs = [docs[item.index] for item in reranked_docs.results]  \n",
    "\n",
    "            \n",
    "            if docs:\n",
    "                response = qa.run(context_query)\n",
    "                print(response)\n",
    "                response = strip_extra_conversation(response)\n",
    "\n",
    "                # Store conversation data\n",
    "                collection2.update_one(\n",
    "                    {\"user_id\": user_id, \"session_id\": key},\n",
    "                    {\"$push\": {\"messages\": {\"role\": \"user\", \"text\": query}}},\n",
    "                    upsert=True\n",
    "                )\n",
    "                collection2.update_one(\n",
    "                    {\"user_id\": user_id, \"session_id\": key},\n",
    "                    {\"$push\": {\"messages\": {\"role\": \"bot\", \"text\": response}}},\n",
    "                    upsert=True\n",
    "                )\n",
    "\n",
    "                return jsonify({\"response\": response})\n",
    "\n",
    "    return jsonify({\"response\": \"No relevant data found.\"})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 🔹 Start an ngrok tunnel with authentication\n",
    "    public_url = ngrok.connect(5000)\n",
    "    print(f\"🔗 Ngrok tunnel established: {public_url}\")\n",
    "\n",
    "    # Run the Flask app\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
